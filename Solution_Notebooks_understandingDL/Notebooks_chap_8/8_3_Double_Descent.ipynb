{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap08/8_3_Double_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6chybAVFJW2"
      },
      "source": [
        "# **Notebook 8.3: Double Descent**\n",
        "\n",
        "This notebook investigates double descent as described in section 8.4 of the book.\n",
        "\n",
        "It uses the MNIST-1D database which can be found at https://github.com/greydanus/mnist1d\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hFxuHpRqTgri"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mnist1d\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Try attaching to GPU -- Use \"Change Runtime Type to change to GPUT\"\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PW2gyXL5UkLU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Did or could not load data from ./mnist1d_data.pkl. Rebuilding dataset...\n",
            "Examples in training set: 4000\n",
            "Examples in test set: 4000\n",
            "Length of each example: 40\n"
          ]
        }
      ],
      "source": [
        "args = mnist1d.data.get_dataset_args()\n",
        "args.num_samples = 8000\n",
        "args.train_split = 0.5\n",
        "args.corr_noise_scale = 0.25\n",
        "args.iid_noise_scale=2e-2\n",
        "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=True)\n",
        "\n",
        "# Add 15% noise to training labels\n",
        "for c_y in range(len(data['y'])):\n",
        "    random_number = random.random()\n",
        "    if random_number < 0.15 :\n",
        "        random_int = int(random.random() * 10)\n",
        "        data['y'][c_y] = random_int\n",
        "\n",
        "# The training and test input and outputs are in\n",
        "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
        "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
        "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
        "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hAIvZOAlTnk9"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters with He initialization\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)\n",
        "\n",
        "# Return an initialized model with two hidden layers and n_hidden hidden units at each\n",
        "def get_model(n_hidden):\n",
        "\n",
        "  D_i = 40    # Input dimensions\n",
        "  D_k = n_hidden   # Hidden dimensions\n",
        "  D_o = 10    # Output dimensions\n",
        "\n",
        "  # Define a model with two hidden layers\n",
        "  # And ReLU activations between them\n",
        "  model = nn.Sequential(\n",
        "  nn.Linear(D_i, D_k),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(D_k, D_k),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(D_k, D_o))\n",
        "\n",
        "  # Call the function you just defined\n",
        "  model.apply(weights_init)\n",
        "\n",
        "  # Return the model\n",
        "  return model ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AazlQhheWmHk"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, data):\n",
        "\n",
        "  # choose cross entropy loss function (equation 5.24)\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  # construct SGD optimizer and initialize learning rate and momentum\n",
        "  # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "  x_train = torch.tensor(data['x'].astype('float32'))\n",
        "  y_train = torch.tensor(data['y'].transpose().astype('long'))\n",
        "  x_test= torch.tensor(data['x_test'].astype('float32'))\n",
        "  y_test = torch.tensor(data['y_test'].astype('long'))\n",
        "\n",
        "  # load the data into a class that creates the batches\n",
        "  data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "  # loop over the dataset n_epoch times\n",
        "  n_epoch = 1000\n",
        "\n",
        "  for epoch in range(n_epoch):\n",
        "    # loop over batches\n",
        "    for i, batch in enumerate(data_loader):\n",
        "      # retrieve inputs and labels for this batch\n",
        "      x_batch, y_batch = batch\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward pass -- calculate model output\n",
        "      pred = model(x_batch)\n",
        "      # compute the loss\n",
        "      loss = loss_function(pred, y_batch.long())\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "      # SGD update\n",
        "      optimizer.step()\n",
        "\n",
        "    # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "    pred_train = model(x_train)\n",
        "    pred_test = model(x_test)\n",
        "    _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "    _, predicted_test_class = torch.max(pred_test.data, 1)\n",
        "    errors_train = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
        "    errors_test = 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)\n",
        "    losses_train = loss_function(pred_train, y_train.long()).item()\n",
        "    losses_test = loss_function(pred_test, y_test.long()).item()\n",
        "    if epoch % 100 == 0:\n",
        "      print(f'Epoch {epoch:5d}, train loss {losses_train:.6f}, train error {errors_train:3.2f},  test loss {losses_test:.6f}, test error {errors_test:3.2f}')\n",
        "\n",
        "  return errors_train, errors_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcP4UPMudxPS"
      },
      "source": [
        "The following code produces the double descent curve by training the model with different numbers of hidden units and plotting the test error.\n",
        "\n",
        "TO DO:\n",
        "\n",
        "*Before* you run the code, and considering that there are 4000 training examples predict:<br>\n",
        "\n",
        "1.    At what capacity do you think the training error will become zero?<br>\n",
        "Rta. The training error will likely become zero when the number of hidden units (model capacity) is sufficiently high.\n",
        "2.   At what capacity do you expect the first minima of the double descent curve to appear?<br>\n",
        "Rta. It typically appears at a capacity where theres a good balance between bias and variance. Might be when the number of hidden units is significantly lower than the treining size.\n",
        "3. At what capacity do you expect the maximum of the double descent curve to appear?<br>\n",
        "Rta. Around 2000-3000 hidden units where the model has enough complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K4OmBZGHWXpk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with   2 hidden variables\n",
            "Epoch     0, train loss 2.266166, train error 85.40,  test loss 2.263010, test error 85.28\n",
            "Epoch   100, train loss 1.904896, train error 71.72,  test loss 1.759827, test error 68.75\n",
            "Epoch   200, train loss 1.888965, train error 70.82,  test loss 1.741710, test error 69.00\n",
            "Epoch   300, train loss 1.863923, train error 70.03,  test loss 1.710547, test error 68.32\n",
            "Epoch   400, train loss 1.849590, train error 70.40,  test loss 1.702828, test error 68.03\n",
            "Epoch   500, train loss 1.848431, train error 70.62,  test loss 1.702299, test error 67.68\n",
            "Epoch   600, train loss 1.842290, train error 70.45,  test loss 1.696132, test error 67.53\n",
            "Epoch   700, train loss 1.840974, train error 69.40,  test loss 1.694744, test error 67.45\n",
            "Epoch   800, train loss 1.840065, train error 70.30,  test loss 1.697904, test error 67.38\n",
            "Epoch   900, train loss 1.849276, train error 69.88,  test loss 1.705830, test error 67.30\n",
            "Training model with   4 hidden variables\n",
            "Epoch     0, train loss 2.177169, train error 81.18,  test loss 2.150219, test error 79.82\n",
            "Epoch   100, train loss 1.781027, train error 65.70,  test loss 1.613911, test error 62.67\n",
            "Epoch   200, train loss 1.762166, train error 64.50,  test loss 1.607000, test error 62.28\n",
            "Epoch   300, train loss 1.745404, train error 63.83,  test loss 1.602411, test error 62.65\n",
            "Epoch   400, train loss 1.706652, train error 62.33,  test loss 1.567848, test error 61.67\n",
            "Epoch   500, train loss 1.702223, train error 62.03,  test loss 1.572273, test error 61.75\n",
            "Epoch   600, train loss 1.701408, train error 62.60,  test loss 1.570493, test error 61.22\n",
            "Epoch   700, train loss 1.701820, train error 62.12,  test loss 1.569574, test error 62.12\n",
            "Epoch   800, train loss 1.700188, train error 62.53,  test loss 1.574290, test error 62.25\n",
            "Epoch   900, train loss 1.702361, train error 62.58,  test loss 1.576354, test error 61.35\n",
            "Training model with   6 hidden variables\n",
            "Epoch     0, train loss 2.268307, train error 85.15,  test loss 2.270780, test error 85.62\n",
            "Epoch   100, train loss 1.671921, train error 58.88,  test loss 1.552041, test error 57.65\n",
            "Epoch   200, train loss 1.632769, train error 57.83,  test loss 1.498937, test error 55.22\n",
            "Epoch   300, train loss 1.626709, train error 57.40,  test loss 1.500684, test error 55.88\n",
            "Epoch   400, train loss 1.620823, train error 56.47,  test loss 1.494217, test error 55.72\n",
            "Epoch   500, train loss 1.615608, train error 57.05,  test loss 1.496252, test error 55.22\n",
            "Epoch   600, train loss 1.616734, train error 57.22,  test loss 1.508138, test error 55.53\n",
            "Epoch   700, train loss 1.608547, train error 56.42,  test loss 1.477825, test error 55.42\n",
            "Epoch   800, train loss 1.600244, train error 56.38,  test loss 1.477686, test error 55.50\n",
            "Epoch   900, train loss 1.596261, train error 55.40,  test loss 1.489637, test error 54.88\n",
            "Training model with   8 hidden variables\n",
            "Epoch     0, train loss 2.288336, train error 86.55,  test loss 2.281085, test error 86.03\n",
            "Epoch   100, train loss 1.626612, train error 58.20,  test loss 1.471860, test error 57.60\n",
            "Epoch   200, train loss 1.576752, train error 55.33,  test loss 1.448713, test error 54.67\n",
            "Epoch   300, train loss 1.555213, train error 54.50,  test loss 1.452754, test error 54.72\n",
            "Epoch   400, train loss 1.560903, train error 54.10,  test loss 1.458137, test error 55.08\n",
            "Epoch   500, train loss 1.548620, train error 53.45,  test loss 1.446253, test error 54.20\n",
            "Epoch   600, train loss 1.553274, train error 54.38,  test loss 1.467902, test error 55.00\n",
            "Epoch   700, train loss 1.547272, train error 53.85,  test loss 1.455383, test error 54.65\n",
            "Epoch   800, train loss 1.546001, train error 53.28,  test loss 1.451513, test error 54.40\n",
            "Epoch   900, train loss 1.552042, train error 54.40,  test loss 1.456904, test error 55.25\n",
            "Training model with  10 hidden variables\n",
            "Epoch     0, train loss 2.209487, train error 82.57,  test loss 2.190557, test error 82.62\n",
            "Epoch   100, train loss 1.573402, train error 53.12,  test loss 1.443996, test error 54.95\n",
            "Epoch   200, train loss 1.534183, train error 53.45,  test loss 1.448803, test error 55.80\n",
            "Epoch   300, train loss 1.511931, train error 52.50,  test loss 1.449185, test error 55.00\n",
            "Epoch   400, train loss 1.485979, train error 51.92,  test loss 1.455408, test error 54.90\n",
            "Epoch   500, train loss 1.465241, train error 51.00,  test loss 1.431411, test error 54.08\n",
            "Epoch   600, train loss 1.455460, train error 50.05,  test loss 1.426988, test error 53.50\n",
            "Epoch   700, train loss 1.452332, train error 49.97,  test loss 1.430779, test error 53.25\n",
            "Epoch   800, train loss 1.440318, train error 50.28,  test loss 1.434796, test error 53.67\n",
            "Epoch   900, train loss 1.439391, train error 50.03,  test loss 1.435882, test error 53.62\n",
            "Training model with  14 hidden variables\n",
            "Epoch     0, train loss 2.219602, train error 82.28,  test loss 2.196412, test error 80.88\n",
            "Epoch   100, train loss 1.432543, train error 48.90,  test loss 1.362269, test error 50.80\n",
            "Epoch   200, train loss 1.348710, train error 46.05,  test loss 1.302362, test error 48.97\n",
            "Epoch   300, train loss 1.321797, train error 45.12,  test loss 1.318711, test error 48.80\n",
            "Epoch   400, train loss 1.300102, train error 44.10,  test loss 1.350578, test error 48.60\n",
            "Epoch   500, train loss 1.284290, train error 44.47,  test loss 1.380130, test error 49.75\n",
            "Epoch   600, train loss 1.276025, train error 42.83,  test loss 1.401051, test error 50.60\n",
            "Epoch   700, train loss 1.268831, train error 43.53,  test loss 1.423213, test error 50.38\n",
            "Epoch   800, train loss 1.271632, train error 42.95,  test loss 1.435864, test error 50.62\n",
            "Epoch   900, train loss 1.259474, train error 42.35,  test loss 1.452488, test error 50.42\n",
            "Training model with  18 hidden variables\n",
            "Epoch     0, train loss 2.164209, train error 80.90,  test loss 2.123383, test error 79.35\n",
            "Epoch   100, train loss 1.388463, train error 46.12,  test loss 1.368423, test error 51.05\n",
            "Epoch   200, train loss 1.270237, train error 41.65,  test loss 1.378099, test error 50.00\n",
            "Epoch   300, train loss 1.231104, train error 40.75,  test loss 1.406356, test error 50.45\n",
            "Epoch   400, train loss 1.194558, train error 40.30,  test loss 1.446381, test error 50.45\n",
            "Epoch   500, train loss 1.179208, train error 39.42,  test loss 1.466131, test error 51.58\n",
            "Epoch   600, train loss 1.170532, train error 39.83,  test loss 1.501741, test error 51.03\n",
            "Epoch   700, train loss 1.159300, train error 39.35,  test loss 1.526377, test error 51.42\n",
            "Epoch   800, train loss 1.156350, train error 39.00,  test loss 1.524740, test error 51.33\n",
            "Epoch   900, train loss 1.144819, train error 38.78,  test loss 1.540422, test error 51.60\n",
            "Training model with  22 hidden variables\n",
            "Epoch     0, train loss 2.243503, train error 82.88,  test loss 2.230910, test error 82.10\n",
            "Epoch   100, train loss 1.246344, train error 40.30,  test loss 1.236644, test error 46.22\n",
            "Epoch   200, train loss 1.129956, train error 36.53,  test loss 1.314615, test error 48.17\n",
            "Epoch   300, train loss 1.091730, train error 35.68,  test loss 1.374259, test error 49.62\n",
            "Epoch   400, train loss 1.053742, train error 35.70,  test loss 1.466211, test error 51.80\n",
            "Epoch   500, train loss 1.028957, train error 34.93,  test loss 1.515543, test error 52.12\n",
            "Epoch   600, train loss 1.006253, train error 34.18,  test loss 1.553181, test error 52.08\n",
            "Epoch   700, train loss 0.999254, train error 34.40,  test loss 1.593505, test error 52.62\n",
            "Epoch   800, train loss 0.985200, train error 33.40,  test loss 1.638123, test error 52.47\n",
            "Epoch   900, train loss 0.966642, train error 32.90,  test loss 1.656199, test error 53.03\n",
            "Training model with  26 hidden variables\n",
            "Epoch     0, train loss 2.139231, train error 79.10,  test loss 2.096478, test error 79.07\n",
            "Epoch   100, train loss 1.174235, train error 37.70,  test loss 1.312941, test error 48.88\n",
            "Epoch   200, train loss 1.038900, train error 34.38,  test loss 1.416927, test error 49.60\n",
            "Epoch   300, train loss 0.974389, train error 33.00,  test loss 1.537793, test error 51.25\n",
            "Epoch   400, train loss 0.927537, train error 30.72,  test loss 1.625787, test error 52.47\n",
            "Epoch   500, train loss 0.898613, train error 29.93,  test loss 1.673598, test error 52.20\n",
            "Epoch   600, train loss 0.883681, train error 29.88,  test loss 1.753969, test error 52.38\n",
            "Epoch   700, train loss 0.847122, train error 28.95,  test loss 1.796845, test error 53.12\n",
            "Epoch   800, train loss 0.840015, train error 29.57,  test loss 1.877189, test error 53.00\n",
            "Epoch   900, train loss 0.813197, train error 27.50,  test loss 1.924633, test error 53.08\n",
            "Training model with  30 hidden variables\n",
            "Epoch     0, train loss 2.172184, train error 80.25,  test loss 2.132008, test error 78.75\n",
            "Epoch   100, train loss 1.129415, train error 36.58,  test loss 1.371407, test error 50.15\n",
            "Epoch   200, train loss 0.957659, train error 31.53,  test loss 1.561842, test error 52.05\n",
            "Epoch   300, train loss 0.863460, train error 29.28,  test loss 1.801209, test error 53.78\n",
            "Epoch   400, train loss 0.813886, train error 26.47,  test loss 1.989461, test error 55.03\n",
            "Epoch   500, train loss 0.780150, train error 25.47,  test loss 2.168468, test error 55.65\n",
            "Epoch   600, train loss 0.766233, train error 25.40,  test loss 2.349926, test error 55.05\n",
            "Epoch   700, train loss 0.736507, train error 24.12,  test loss 2.449322, test error 55.83\n",
            "Epoch   800, train loss 0.725582, train error 23.95,  test loss 2.559272, test error 56.45\n",
            "Epoch   900, train loss 0.730572, train error 24.00,  test loss 2.708538, test error 56.00\n",
            "Training model with  35 hidden variables\n",
            "Epoch     0, train loss 2.172069, train error 81.53,  test loss 2.169534, test error 81.38\n",
            "Epoch   100, train loss 0.993715, train error 31.78,  test loss 1.425416, test error 50.53\n",
            "Epoch   200, train loss 0.781352, train error 24.75,  test loss 1.822701, test error 53.17\n",
            "Epoch   300, train loss 0.668631, train error 22.10,  test loss 2.165412, test error 53.88\n",
            "Epoch   400, train loss 0.592313, train error 19.72,  test loss 2.578778, test error 55.55\n",
            "Epoch   500, train loss 0.564468, train error 19.78,  test loss 2.972654, test error 56.78\n",
            "Epoch   600, train loss 0.527781, train error 18.20,  test loss 3.205055, test error 56.12\n",
            "Epoch   700, train loss 0.480908, train error 16.28,  test loss 3.566914, test error 57.30\n",
            "Epoch   800, train loss 0.449165, train error 15.45,  test loss 3.897569, test error 58.10\n",
            "Epoch   900, train loss 0.442847, train error 15.47,  test loss 4.212135, test error 58.30\n",
            "Training model with  40 hidden variables\n",
            "Epoch     0, train loss 2.139832, train error 78.20,  test loss 2.118696, test error 77.90\n",
            "Epoch   100, train loss 0.943461, train error 30.28,  test loss 1.394438, test error 50.00\n",
            "Epoch   200, train loss 0.658703, train error 21.30,  test loss 1.891931, test error 53.08\n",
            "Epoch   300, train loss 0.480026, train error 15.40,  test loss 2.492297, test error 53.83\n",
            "Epoch   400, train loss 0.390758, train error 11.82,  test loss 3.221623, test error 54.95\n",
            "Epoch   500, train loss 0.319753, train error 10.03,  test loss 3.902672, test error 55.58\n",
            "Epoch   600, train loss 0.284660, train error 9.18,  test loss 4.529090, test error 54.62\n",
            "Epoch   700, train loss 0.266264, train error 8.35,  test loss 5.112144, test error 54.12\n",
            "Epoch   800, train loss 0.262529, train error 9.03,  test loss 5.836900, test error 54.78\n",
            "Epoch   900, train loss 0.252024, train error 9.35,  test loss 6.301216, test error 54.88\n",
            "Training model with  45 hidden variables\n",
            "Epoch     0, train loss 2.109961, train error 78.32,  test loss 2.075445, test error 76.70\n",
            "Epoch   100, train loss 0.809806, train error 25.62,  test loss 1.534058, test error 50.45\n",
            "Epoch   200, train loss 0.503352, train error 16.50,  test loss 2.287737, test error 52.88\n",
            "Epoch   300, train loss 0.321038, train error 10.57,  test loss 3.383462, test error 54.85\n",
            "Epoch   400, train loss 0.230923, train error 7.38,  test loss 4.676591, test error 56.15\n",
            "Epoch   500, train loss 0.173983, train error 5.55,  test loss 5.860200, test error 55.72\n",
            "Epoch   600, train loss 0.121543, train error 3.28,  test loss 7.272933, test error 57.05\n",
            "Epoch   700, train loss 0.016600, train error 0.00,  test loss 8.371293, test error 57.83\n",
            "Epoch   800, train loss 0.009991, train error 0.00,  test loss 9.155551, test error 57.83\n",
            "Epoch   900, train loss 0.007351, train error 0.00,  test loss 9.743695, test error 58.05\n",
            "Training model with  50 hidden variables\n",
            "Epoch     0, train loss 2.082878, train error 78.80,  test loss 2.042963, test error 78.05\n",
            "Epoch   100, train loss 0.673258, train error 20.35,  test loss 1.547299, test error 50.83\n",
            "Epoch   200, train loss 0.309025, train error 9.03,  test loss 2.672595, test error 54.65\n",
            "Epoch   300, train loss 0.122401, train error 2.45,  test loss 4.205281, test error 55.67\n",
            "Epoch   400, train loss 0.027893, train error 0.00,  test loss 5.675553, test error 55.85\n",
            "Epoch   500, train loss 0.013663, train error 0.00,  test loss 6.585834, test error 56.30\n",
            "Epoch   600, train loss 0.008726, train error 0.00,  test loss 7.209996, test error 56.53\n",
            "Epoch   700, train loss 0.006209, train error 0.00,  test loss 7.670008, test error 56.30\n",
            "Epoch   800, train loss 0.004730, train error 0.00,  test loss 8.036274, test error 56.35\n",
            "Epoch   900, train loss 0.003813, train error 0.00,  test loss 8.342717, test error 56.40\n",
            "Training model with  55 hidden variables\n",
            "Epoch     0, train loss 2.040276, train error 75.32,  test loss 1.960145, test error 73.75\n",
            "Epoch   100, train loss 0.634714, train error 18.15,  test loss 1.707262, test error 52.60\n",
            "Epoch   200, train loss 0.254755, train error 6.68,  test loss 3.175377, test error 56.05\n",
            "Epoch   300, train loss 0.043236, train error 0.03,  test loss 4.985385, test error 56.80\n",
            "Epoch   400, train loss 0.014525, train error 0.00,  test loss 6.134513, test error 57.12\n",
            "Epoch   500, train loss 0.008290, train error 0.00,  test loss 6.781666, test error 57.22\n",
            "Epoch   600, train loss 0.005627, train error 0.00,  test loss 7.254267, test error 57.25\n",
            "Epoch   700, train loss 0.004221, train error 0.00,  test loss 7.624389, test error 57.33\n",
            "Epoch   800, train loss 0.003330, train error 0.00,  test loss 7.916406, test error 57.25\n",
            "Epoch   900, train loss 0.002734, train error 0.00,  test loss 8.169304, test error 57.17\n",
            "Training model with  60 hidden variables\n",
            "Epoch     0, train loss 2.079700, train error 77.18,  test loss 2.021053, test error 76.55\n",
            "Epoch   100, train loss 0.543951, train error 15.45,  test loss 1.713252, test error 51.12\n",
            "Epoch   200, train loss 0.124636, train error 1.78,  test loss 3.266775, test error 52.95\n",
            "Epoch   300, train loss 0.019808, train error 0.00,  test loss 4.613633, test error 53.75\n",
            "Epoch   400, train loss 0.009446, train error 0.00,  test loss 5.284206, test error 53.45\n",
            "Epoch   500, train loss 0.005927, train error 0.00,  test loss 5.716118, test error 53.55\n",
            "Epoch   600, train loss 0.004239, train error 0.00,  test loss 6.039701, test error 53.72\n",
            "Epoch   700, train loss 0.003275, train error 0.00,  test loss 6.291477, test error 53.75\n",
            "Epoch   800, train loss 0.002618, train error 0.00,  test loss 6.505136, test error 53.80\n",
            "Epoch   900, train loss 0.002181, train error 0.00,  test loss 6.687542, test error 53.72\n",
            "Training model with  70 hidden variables\n",
            "Epoch     0, train loss 2.045091, train error 75.47,  test loss 2.014788, test error 75.40\n",
            "Epoch   100, train loss 0.398393, train error 10.00,  test loss 1.920406, test error 53.20\n",
            "Epoch   200, train loss 0.038065, train error 0.00,  test loss 3.629925, test error 55.85\n",
            "Epoch   300, train loss 0.011626, train error 0.00,  test loss 4.477457, test error 55.67\n",
            "Epoch   400, train loss 0.006450, train error 0.00,  test loss 4.953099, test error 55.80\n",
            "Epoch   500, train loss 0.004291, train error 0.00,  test loss 5.284108, test error 55.97\n",
            "Epoch   600, train loss 0.003159, train error 0.00,  test loss 5.530116, test error 55.78\n",
            "Epoch   700, train loss 0.002473, train error 0.00,  test loss 5.731980, test error 55.80\n",
            "Epoch   800, train loss 0.002019, train error 0.00,  test loss 5.899077, test error 55.85\n",
            "Epoch   900, train loss 0.001693, train error 0.00,  test loss 6.043089, test error 55.85\n",
            "Training model with  80 hidden variables\n",
            "Epoch     0, train loss 2.044328, train error 75.05,  test loss 1.984571, test error 76.05\n",
            "Epoch   100, train loss 0.252621, train error 5.12,  test loss 2.100024, test error 52.40\n",
            "Epoch   200, train loss 0.020777, train error 0.00,  test loss 3.457285, test error 52.72\n",
            "Epoch   300, train loss 0.008248, train error 0.00,  test loss 4.029284, test error 52.72\n",
            "Epoch   400, train loss 0.004868, train error 0.00,  test loss 4.361881, test error 52.62\n",
            "Epoch   500, train loss 0.003351, train error 0.00,  test loss 4.603885, test error 52.45\n",
            "Epoch   600, train loss 0.002518, train error 0.00,  test loss 4.786306, test error 52.55\n",
            "Epoch   700, train loss 0.001999, train error 0.00,  test loss 4.932815, test error 52.50\n",
            "Epoch   800, train loss 0.001643, train error 0.00,  test loss 5.061115, test error 52.55\n",
            "Epoch   900, train loss 0.001389, train error 0.00,  test loss 5.170776, test error 52.30\n",
            "Training model with  90 hidden variables\n",
            "Epoch     0, train loss 1.984447, train error 73.97,  test loss 1.909256, test error 73.78\n",
            "Epoch   100, train loss 0.142984, train error 1.35,  test loss 2.157775, test error 50.30\n",
            "Epoch   200, train loss 0.014907, train error 0.00,  test loss 3.177006, test error 50.85\n",
            "Epoch   300, train loss 0.006570, train error 0.00,  test loss 3.591162, test error 50.53\n",
            "Epoch   400, train loss 0.004005, train error 0.00,  test loss 3.846437, test error 50.90\n",
            "Epoch   500, train loss 0.002815, train error 0.00,  test loss 4.033863, test error 50.97\n",
            "Epoch   600, train loss 0.002141, train error 0.00,  test loss 4.178696, test error 51.08\n",
            "Epoch   700, train loss 0.001711, train error 0.00,  test loss 4.297705, test error 51.12\n",
            "Epoch   800, train loss 0.001416, train error 0.00,  test loss 4.398673, test error 51.22\n",
            "Epoch   900, train loss 0.001202, train error 0.00,  test loss 4.485296, test error 51.35\n",
            "Training model with 100 hidden variables\n",
            "Epoch     0, train loss 1.994838, train error 73.28,  test loss 1.939258, test error 73.68\n",
            "Epoch   100, train loss 0.098264, train error 0.15,  test loss 2.290231, test error 51.55\n",
            "Epoch   200, train loss 0.012578, train error 0.00,  test loss 3.235827, test error 51.65\n",
            "Epoch   300, train loss 0.005828, train error 0.00,  test loss 3.626476, test error 51.45\n",
            "Epoch   400, train loss 0.003603, train error 0.00,  test loss 3.871764, test error 51.53\n",
            "Epoch   500, train loss 0.002558, train error 0.00,  test loss 4.049449, test error 51.53\n",
            "Epoch   600, train loss 0.001957, train error 0.00,  test loss 4.189728, test error 51.53\n",
            "Epoch   700, train loss 0.001572, train error 0.00,  test loss 4.306043, test error 51.55\n",
            "Epoch   800, train loss 0.001306, train error 0.00,  test loss 4.402414, test error 51.53\n",
            "Epoch   900, train loss 0.001112, train error 0.00,  test loss 4.488044, test error 51.70\n",
            "Training model with 120 hidden variables\n",
            "Epoch     0, train loss 1.973292, train error 73.55,  test loss 1.909139, test error 74.70\n",
            "Epoch   100, train loss 0.049982, train error 0.00,  test loss 2.240448, test error 51.08\n",
            "Epoch   200, train loss 0.009479, train error 0.00,  test loss 2.868916, test error 51.10\n",
            "Epoch   300, train loss 0.004699, train error 0.00,  test loss 3.146777, test error 51.20\n",
            "Epoch   400, train loss 0.003012, train error 0.00,  test loss 3.326629, test error 51.17\n",
            "Epoch   500, train loss 0.002175, train error 0.00,  test loss 3.460358, test error 51.33\n",
            "Epoch   600, train loss 0.001683, train error 0.00,  test loss 3.567738, test error 51.05\n",
            "Epoch   700, train loss 0.001360, train error 0.00,  test loss 3.654414, test error 51.08\n",
            "Epoch   800, train loss 0.001135, train error 0.00,  test loss 3.728276, test error 51.08\n",
            "Epoch   900, train loss 0.000970, train error 0.00,  test loss 3.793148, test error 50.97\n",
            "Training model with 140 hidden variables\n",
            "Epoch     0, train loss 1.952666, train error 72.75,  test loss 1.885480, test error 72.25\n",
            "Epoch   100, train loss 0.036276, train error 0.00,  test loss 2.178901, test error 49.92\n",
            "Epoch   200, train loss 0.007963, train error 0.00,  test loss 2.670955, test error 50.22\n",
            "Epoch   300, train loss 0.004093, train error 0.00,  test loss 2.908481, test error 50.12\n",
            "Epoch   400, train loss 0.002660, train error 0.00,  test loss 3.060676, test error 50.17\n",
            "Epoch   500, train loss 0.001935, train error 0.00,  test loss 3.174493, test error 50.20\n",
            "Epoch   600, train loss 0.001503, train error 0.00,  test loss 3.264492, test error 50.12\n",
            "Epoch   700, train loss 0.001221, train error 0.00,  test loss 3.338964, test error 50.12\n",
            "Epoch   800, train loss 0.001022, train error 0.00,  test loss 3.403091, test error 50.15\n",
            "Epoch   900, train loss 0.000876, train error 0.00,  test loss 3.458529, test error 50.25\n",
            "Training model with 160 hidden variables\n",
            "Epoch     0, train loss 1.952541, train error 71.80,  test loss 1.859284, test error 72.85\n",
            "Epoch   100, train loss 0.028511, train error 0.00,  test loss 2.114061, test error 48.45\n",
            "Epoch   200, train loss 0.007189, train error 0.00,  test loss 2.534379, test error 48.72\n",
            "Epoch   300, train loss 0.003792, train error 0.00,  test loss 2.734343, test error 48.60\n",
            "Epoch   400, train loss 0.002493, train error 0.00,  test loss 2.866772, test error 48.42\n",
            "Epoch   500, train loss 0.001823, train error 0.00,  test loss 2.967883, test error 48.72\n",
            "Epoch   600, train loss 0.001422, train error 0.00,  test loss 3.046602, test error 48.65\n",
            "Epoch   700, train loss 0.001157, train error 0.00,  test loss 3.112630, test error 48.65\n",
            "Epoch   800, train loss 0.000971, train error 0.00,  test loss 3.169227, test error 48.72\n",
            "Epoch   900, train loss 0.000833, train error 0.00,  test loss 3.218375, test error 48.78\n",
            "Training model with 180 hidden variables\n",
            "Epoch     0, train loss 1.930953, train error 71.93,  test loss 1.870334, test error 72.97\n",
            "Epoch   100, train loss 0.024420, train error 0.00,  test loss 2.022171, test error 48.45\n",
            "Epoch   200, train loss 0.006609, train error 0.00,  test loss 2.380731, test error 48.75\n",
            "Epoch   300, train loss 0.003513, train error 0.00,  test loss 2.562324, test error 48.65\n",
            "Epoch   400, train loss 0.002325, train error 0.00,  test loss 2.683656, test error 48.80\n",
            "Epoch   500, train loss 0.001711, train error 0.00,  test loss 2.774189, test error 48.78\n",
            "Epoch   600, train loss 0.001340, train error 0.00,  test loss 2.846936, test error 48.85\n",
            "Epoch   700, train loss 0.001094, train error 0.00,  test loss 2.907541, test error 48.90\n",
            "Epoch   800, train loss 0.000920, train error 0.00,  test loss 2.959921, test error 48.83\n",
            "Epoch   900, train loss 0.000791, train error 0.00,  test loss 3.005322, test error 48.80\n",
            "Training model with 200 hidden variables\n",
            "Epoch     0, train loss 1.922353, train error 71.40,  test loss 1.867843, test error 73.00\n",
            "Epoch   100, train loss 0.021557, train error 0.00,  test loss 2.028216, test error 47.95\n",
            "Epoch   200, train loss 0.006156, train error 0.00,  test loss 2.347978, test error 48.47\n",
            "Epoch   300, train loss 0.003339, train error 0.00,  test loss 2.511175, test error 48.25\n",
            "Epoch   400, train loss 0.002225, train error 0.00,  test loss 2.621388, test error 48.28\n",
            "Epoch   500, train loss 0.001643, train error 0.00,  test loss 2.702926, test error 48.42\n",
            "Epoch   600, train loss 0.001290, train error 0.00,  test loss 2.770743, test error 48.35\n",
            "Epoch   700, train loss 0.001055, train error 0.00,  test loss 2.826580, test error 48.22\n",
            "Epoch   800, train loss 0.000889, train error 0.00,  test loss 2.874337, test error 48.25\n",
            "Epoch   900, train loss 0.000765, train error 0.00,  test loss 2.915729, test error 48.42\n",
            "Training model with 250 hidden variables\n",
            "Epoch     0, train loss 1.893496, train error 67.93,  test loss 1.816368, test error 70.50\n",
            "Epoch   100, train loss 0.017310, train error 0.00,  test loss 1.900264, test error 46.58\n",
            "Epoch   200, train loss 0.005387, train error 0.00,  test loss 2.148778, test error 46.92\n",
            "Epoch   300, train loss 0.002996, train error 0.00,  test loss 2.283292, test error 46.78\n",
            "Epoch   400, train loss 0.002022, train error 0.00,  test loss 2.375729, test error 46.70\n",
            "Epoch   500, train loss 0.001504, train error 0.00,  test loss 2.445724, test error 46.60\n",
            "Epoch   600, train loss 0.001187, train error 0.00,  test loss 2.502556, test error 46.70\n",
            "Epoch   700, train loss 0.000975, train error 0.00,  test loss 2.550362, test error 46.62\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(hidden_variables[c_hidden])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m errors_train, errors_test \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Store the results\u001b[39;00m\n\u001b[0;32m     15\u001b[0m errors_train_all[c_hidden] \u001b[38;5;241m=\u001b[39m errors_train\n",
            "Cell \u001b[1;32mIn[9], line 33\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(pred, y_batch\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# SGD update\u001b[39;00m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\laura\\OneDrive\\Documentos\\GitHub\\Understanding-DL---MPAM\\venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\laura\\OneDrive\\Documentos\\GitHub\\Understanding-DL---MPAM\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\laura\\OneDrive\\Documentos\\GitHub\\Understanding-DL---MPAM\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# This code will take a while (~30 mins on GPU) to run!  Go and make a cup of coffee!\n",
        "\n",
        "hidden_variables = np.array([2,4,6,8,10,14,18,22,26,30,35,40,45,50,55,60,70,80,90,100,120,140,160,180,200,250,300,400]) ;\n",
        "errors_train_all = np.zeros_like(hidden_variables)\n",
        "errors_test_all = np.zeros_like(hidden_variables)\n",
        "\n",
        "# For each hidden variable size\n",
        "for c_hidden in range(len(hidden_variables)):\n",
        "    print(f'Training model with {hidden_variables[c_hidden]:3d} hidden variables')\n",
        "    # Get a model\n",
        "    model = get_model(hidden_variables[c_hidden])\n",
        "    # Train the model\n",
        "    errors_train, errors_test = fit_model(model, data)\n",
        "    # Store the results\n",
        "    errors_train_all[c_hidden] = errors_train\n",
        "    errors_test_all[c_hidden] = errors_test\n",
        "\n",
        "#Interrumpo el proceso en el tiempo 76m 50.3s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rw-iRboTXbck"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(hidden_variables, errors_train_all,'r-',label='train')\n",
        "ax.plot(hidden_variables, errors_test_all,'b-',label='test')\n",
        "ax.set_ylim(0,100);\n",
        "ax.set_xlabel('No hidden variables'); ax.set_ylabel('Error')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap07/7_3_Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6chybAVFJW2"
      },
      "source": [
        "# **Notebook 7.3: Initialization**\n",
        "\n",
        "This notebook explores weight initialization in deep neural networks as described in section 7.5 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LdIDglk1FFcG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnUoI0m6GyjC"
      },
      "source": [
        "First let's define a neural network.  We'll just choose the weights and biases randomly for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WVM4Tc_jGI0Q"
      },
      "outputs": [],
      "source": [
        "def init_params(K, D, sigma_sq_omega):\n",
        "  # Set seed so we always get the same random numbers\n",
        "  np.random.seed(0)\n",
        "\n",
        "  # Input layer\n",
        "  D_i = 1\n",
        "  # Output layer\n",
        "  D_o = 1\n",
        "\n",
        "  # Make empty lists\n",
        "  all_weights = [None] * (K+1)\n",
        "  all_biases = [None] * (K+1)\n",
        "\n",
        "  # Create input and output layers\n",
        "  all_weights[0] = np.random.normal(size=(D, D_i))*np.sqrt(sigma_sq_omega)\n",
        "  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)\n",
        "  all_biases[0] = np.zeros((D,1))\n",
        "  all_biases[-1]= np.zeros((D_o,1))\n",
        "\n",
        "  # Create intermediate layers\n",
        "  for layer in range(1,K):\n",
        "    all_weights[layer] = np.random.normal(size=(D,D))*np.sqrt(sigma_sq_omega)\n",
        "    all_biases[layer] = np.zeros((D,1))\n",
        "\n",
        "  return all_weights, all_biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jZh-7bPXIDq4"
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LgquJUJvJPaN"
      },
      "outputs": [],
      "source": [
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "\n",
        "  # Retrieve number of layers\n",
        "  K = len(all_weights)-1\n",
        "\n",
        "  # We'll store the pre-activations at each layer in a list \"all_f\"\n",
        "  # and the activations in a second list \"all_h\".\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "\n",
        "  #For convenience, we'll set\n",
        "  # all_h[0] to be the input, and all_f[K] will be the output\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
        "  for layer in range(K):\n",
        "      # Update preactivations and activations at this layer according to eqn 7.5\n",
        "      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n",
        "      all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "  # Compute the output from the last hidden layer\n",
        "  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n",
        "\n",
        "  # Retrieve the output\n",
        "  net_output = all_f[K]\n",
        "\n",
        "  return net_output, all_f, all_h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIUrcXnOqChl"
      },
      "source": [
        "Now let's investigate how the size of the outputs vary as we change the initialization variance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A55z3rKBqO7M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1, std of hidden units = 0.811\n",
            "Layer 2, std of hidden units = 1.472\n",
            "Layer 3, std of hidden units = 4.547\n",
            "Layer 4, std of hidden units = 8.896\n",
            "Layer 5, std of hidden units = 10.106\n"
          ]
        }
      ],
      "source": [
        "# Number of layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 8\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "# Set variance of initial weights to 1\n",
        "sigma_sq_omega = 1.0\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
        "\n",
        "n_data = 1000\n",
        "data_in = np.random.normal(size=(1,n_data))\n",
        "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
        "\n",
        "for layer in range(1,K+1):\n",
        "  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VL_SO4tar3DC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1, std of hidden units = 0.622\n",
            "Layer 2, std of hidden units = 3.108\n",
            "Layer 3, std of hidden units = 21.075\n",
            "Layer 4, std of hidden units = 161.638\n",
            "Layer 5, std of hidden units = 1125.582\n",
            "Layer 6, std of hidden units = 6319.072\n",
            "Layer 7, std of hidden units = 37275.665\n",
            "Layer 8, std of hidden units = 243387.814\n",
            "Layer 9, std of hidden units = 1339835.231\n",
            "Layer 10, std of hidden units = 7366234.399\n",
            "Layer 11, std of hidden units = 49006173.785\n",
            "Layer 12, std of hidden units = 272845366.658\n",
            "Layer 13, std of hidden units = 1682043584.115\n",
            "Layer 14, std of hidden units = 10666632256.715\n",
            "Layer 15, std of hidden units = 66098343304.232\n",
            "Layer 16, std of hidden units = 429669007251.536\n",
            "Layer 17, std of hidden units = 2889209957356.916\n",
            "Layer 18, std of hidden units = 19621779417283.500\n",
            "Layer 19, std of hidden units = 121787762396578.984\n",
            "Layer 20, std of hidden units = 999886829483868.875\n",
            "Layer 21, std of hidden units = 5334411928004678.000\n",
            "Layer 22, std of hidden units = 33827620837739412.000\n",
            "Layer 23, std of hidden units = 225444894681278176.000\n",
            "Layer 24, std of hidden units = 1627820610460267776.000\n",
            "Layer 25, std of hidden units = 11267764649765797888.000\n",
            "Layer 26, std of hidden units = 67624804921841565696.000\n",
            "Layer 27, std of hidden units = 364972590784171016192.000\n",
            "Layer 28, std of hidden units = 2240665662851632070656.000\n",
            "Layer 29, std of hidden units = 17591855121671590510592.000\n",
            "Layer 30, std of hidden units = 127571735900692099891200.000\n",
            "Layer 31, std of hidden units = 673191222755367401291776.000\n",
            "Layer 32, std of hidden units = 3013965906952738308096000.000\n",
            "Layer 33, std of hidden units = 18058080219374338489450496.000\n",
            "Layer 34, std of hidden units = 113844343088523883136942080.000\n",
            "Layer 35, std of hidden units = 743768651021983614777688064.000\n",
            "Layer 36, std of hidden units = 4212183983909334019905421312.000\n",
            "Layer 37, std of hidden units = 29165435896872114952769372160.000\n",
            "Layer 38, std of hidden units = 158335947919997755737983942656.000\n",
            "Layer 39, std of hidden units = 997715185910698111272413560832.000\n",
            "Layer 40, std of hidden units = 6426783972194131163949876379648.000\n",
            "Layer 41, std of hidden units = 43768309206637147921539531800576.000\n",
            "Layer 42, std of hidden units = 303487129329508587245138174017536.000\n",
            "Layer 43, std of hidden units = 1776138313453572474864744603320320.000\n",
            "Layer 44, std of hidden units = 11908465022449954960409252809670656.000\n",
            "Layer 45, std of hidden units = 89231570537681377058251182172012544.000\n",
            "Layer 46, std of hidden units = 537142295281509613087075230529093632.000\n",
            "Layer 47, std of hidden units = 3598219068834594188168289796459855872.000\n",
            "Layer 48, std of hidden units = 19195733181303615269323687454815813632.000\n",
            "Layer 49, std of hidden units = 112209168636519545406961012606813339648.000\n",
            "Layer 50, std of hidden units = 753218502856424628936041819469258948608.000\n",
            "------------------------------------------------------------------------------------------------\n",
            "Layer 1, std of hidden units = 0.197  Variance of weights = 0.098\n",
            "Layer 2, std of hidden units = 0.311  Variance of weights = 0.099\n",
            "Layer 3, std of hidden units = 0.666  Variance of weights = 0.097\n",
            "Layer 4, std of hidden units = 1.616  Variance of weights = 0.099\n",
            "Layer 5, std of hidden units = 3.559  Variance of weights = 0.102\n",
            "Layer 6, std of hidden units = 6.319  Variance of weights = 0.101\n",
            "Layer 7, std of hidden units = 11.788  Variance of weights = 0.099\n",
            "Layer 8, std of hidden units = 24.339  Variance of weights = 0.101\n",
            "Layer 9, std of hidden units = 42.369  Variance of weights = 0.100\n",
            "Layer 10, std of hidden units = 73.662  Variance of weights = 0.095\n",
            "Layer 11, std of hidden units = 154.971  Variance of weights = 0.101\n",
            "Layer 12, std of hidden units = 272.845  Variance of weights = 0.099\n",
            "Layer 13, std of hidden units = 531.909  Variance of weights = 0.101\n",
            "Layer 14, std of hidden units = 1066.663  Variance of weights = 0.100\n",
            "Layer 15, std of hidden units = 2090.213  Variance of weights = 0.101\n",
            "Layer 16, std of hidden units = 4296.690  Variance of weights = 0.098\n",
            "Layer 17, std of hidden units = 9136.484  Variance of weights = 0.101\n",
            "Layer 18, std of hidden units = 19621.779  Variance of weights = 0.098\n",
            "Layer 19, std of hidden units = 38512.672  Variance of weights = 0.102\n",
            "Layer 20, std of hidden units = 99988.683  Variance of weights = 0.099\n",
            "Layer 21, std of hidden units = 168688.917  Variance of weights = 0.099\n",
            "Layer 22, std of hidden units = 338276.208  Variance of weights = 0.100\n",
            "Layer 23, std of hidden units = 712919.354  Variance of weights = 0.097\n",
            "Layer 24, std of hidden units = 1627820.610  Variance of weights = 0.100\n",
            "Layer 25, std of hidden units = 3563180.043  Variance of weights = 0.100\n",
            "Layer 26, std of hidden units = 6762480.492  Variance of weights = 0.101\n",
            "Layer 27, std of hidden units = 11541446.704  Variance of weights = 0.099\n",
            "Layer 28, std of hidden units = 22406656.629  Variance of weights = 0.098\n",
            "Layer 29, std of hidden units = 55630330.452  Variance of weights = 0.100\n",
            "Layer 30, std of hidden units = 127571735.901  Variance of weights = 0.102\n",
            "Layer 31, std of hidden units = 212881756.474  Variance of weights = 0.101\n",
            "Layer 32, std of hidden units = 301396590.695  Variance of weights = 0.099\n",
            "Layer 33, std of hidden units = 571046636.633  Variance of weights = 0.101\n",
            "Layer 34, std of hidden units = 1138443430.885  Variance of weights = 0.099\n",
            "Layer 35, std of hidden units = 2352002989.460  Variance of weights = 0.101\n",
            "Layer 36, std of hidden units = 4212183983.909  Variance of weights = 0.098\n",
            "Layer 37, std of hidden units = 9222920638.575  Variance of weights = 0.098\n",
            "Layer 38, std of hidden units = 15833594792.000  Variance of weights = 0.100\n",
            "Layer 39, std of hidden units = 31550524436.161  Variance of weights = 0.100\n",
            "Layer 40, std of hidden units = 64267839721.941  Variance of weights = 0.102\n",
            "Layer 41, std of hidden units = 138407546427.491  Variance of weights = 0.100\n",
            "Layer 42, std of hidden units = 303487129329.509  Variance of weights = 0.098\n",
            "Layer 43, std of hidden units = 561664251000.338  Variance of weights = 0.101\n",
            "Layer 44, std of hidden units = 1190846502244.997  Variance of weights = 0.098\n",
            "Layer 45, std of hidden units = 2821750020930.491  Variance of weights = 0.103\n",
            "Layer 46, std of hidden units = 5371422952815.104  Variance of weights = 0.101\n",
            "Layer 47, std of hidden units = 11378567777767.520  Variance of weights = 0.102\n",
            "Layer 48, std of hidden units = 19195733181303.641  Variance of weights = 0.101\n",
            "Layer 49, std of hidden units = 35483654724533.266  Variance of weights = 0.100\n",
            "Layer 50, std of hidden units = 75321850285642.578  Variance of weights = 0.102\n"
          ]
        }
      ],
      "source": [
        "# You can see that the values of the hidden units are increasing on average (the variance is across all hidden units at the layer\n",
        "# and the 1000 training examples\n",
        "\n",
        "# TO DO\n",
        "# Change this to 50 layers with 80 hidden units per layer\n",
        "# Number of layers\n",
        "K = 50\n",
        "# Number of neurons per layer\n",
        "D = 80\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "# Set variance of initial weights to 1\n",
        "sigma_sq_omega = 1.0\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
        "\n",
        "n_data = 1000\n",
        "data_in = np.random.normal(size=(1,n_data))\n",
        "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
        "\n",
        "for layer in range(1,K+1):\n",
        "  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))\n",
        "  \n",
        "\n",
        "\n",
        "print(\"------------------------\"*4)\n",
        "\n",
        "# TO DO\n",
        "# Now experiment with sigma_sq_omega to try to stop the variance of the forward computation exploding\n",
        "# Number of layers\n",
        "K = 50\n",
        "# Number of neurons per layer\n",
        "D = 80\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "# Set variance of initial weights to 1\n",
        "sigma_sq_omega = 0.1\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
        "\n",
        "n_data = 1000\n",
        "data_in = np.random.normal(size=(1,n_data))\n",
        "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
        "\n",
        "for layer in range(1,K+1):\n",
        "  weights_var = np.var(all_weights[layer])\n",
        "  print(\"Layer %d, std of hidden units = %3.3f  Variance of weights = %3.3f\" %(layer, np.std(all_h[layer]), weights_var))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxVTKp3IcoBF"
      },
      "source": [
        "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6XqWSYWJdhQR"
      },
      "outputs": [],
      "source": [
        "def least_squares_loss(net_output, y):\n",
        "  return np.sum((net_output-y) * (net_output-y))\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "    return 2*(net_output -y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WmyqFYWA-0"
      },
      "source": [
        "Here's the code for the backward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LJng7WpRPLMz"
      },
      "outputs": [],
      "source": [
        "# We'll need the indicator function\n",
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>=0] = 1\n",
        "  x_in[x_in<0] = 0\n",
        "  return x_in\n",
        "\n",
        "# Main backward pass routine\n",
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "  # Retrieve number of layers\n",
        "  K = len(all_weights) - 1\n",
        "\n",
        "  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases = [None] * (K+1)\n",
        "  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n",
        "  all_dl_df = [None] * (K+1)\n",
        "  all_dl_dh = [None] * (K+1)\n",
        "  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n",
        "\n",
        "  # Compute derivatives of net output with respect to loss\n",
        "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
        "\n",
        "  # Now work backwards through the network\n",
        "  for layer in range(K,-1,-1):\n",
        "    # Calculate the derivatives of biases at layer from all_dl_df[K]. (eq 7.13, line 1)\n",
        "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
        "    # Calculate the derivatives of weight at layer from all_dl_df[K] and all_h[K] (eq 7.13, line 2)\n",
        "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].transpose())\n",
        "\n",
        "    # Calculate the derivatives of activations from weight and derivatives of next preactivations (eq 7.13, line 3 second part)\n",
        "    all_dl_dh[layer] = np.matmul(all_weights[layer].transpose(), all_dl_df[layer])\n",
        "    # Calculate the derivatives of the pre-activation f with respect to activation h (eq 7.13, line 3, first part)\n",
        "    if layer > 0:\n",
        "      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]\n",
        "\n",
        "  return all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phFnbthqwhFi"
      },
      "source": [
        "Now let's look at what happens to the magnitude of the gradients on the way back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9A9MHc4sQvbp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1, std of dl_dh = 446.654\n",
            "Layer 2, std of dl_dh = 340.657\n",
            "Layer 3, std of dl_dh = 109.132\n",
            "Layer 4, std of dl_dh = 56.472\n"
          ]
        }
      ],
      "source": [
        "# Number of layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 8\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "# Set variance of initial weights to 1\n",
        "sigma_sq_omega = 1.0\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
        "\n",
        "# For simplicity we'll just consider the gradients of the weights and biases between the first and last hidden layer\n",
        "n_data = 100\n",
        "aggregate_dl_df = [None] * (K+1)\n",
        "for layer in range(1,K):\n",
        "  # These 3D arrays will store the gradients for every data point\n",
        "  aggregate_dl_df[layer] = np.zeros((D,n_data))\n",
        "\n",
        "\n",
        "# We'll have to compute the derivatives of the parameters for each data point separately\n",
        "for c_data in range(n_data):\n",
        "  data_in = np.random.normal(size=(1,1))\n",
        "  y = np.zeros((1,1))\n",
        "  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
        "  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
        "  for layer in range(1,K):\n",
        "    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])\n",
        "\n",
        "for layer in range(1,K):\n",
        "  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gtokc0VX0839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1, std of dl_dh = 3864161615668242735956710189167862268084991549035122385352428584897045577334784.000\n",
            "Layer 2, std of dl_dh = 644297462740804610598547067451302999122676082898034536151030348335676055879680.000\n",
            "Layer 3, std of dl_dh = 116256578506420923819026128549181869661227298447599782554315757230354610520064.000\n",
            "Layer 4, std of dl_dh = 18012406883088290574820624800592506146677260025062386448774316977284251123712.000\n",
            "Layer 5, std of dl_dh = 2657382923492155639252254616520408642680996766242234478034944863745840513024.000\n",
            "Layer 6, std of dl_dh = 437115574218245193807908837898776765075349933300781174644617597538000699392.000\n",
            "Layer 7, std of dl_dh = 72289425689022383694959922502133948966481633180885036698402715526890520576.000\n",
            "Layer 8, std of dl_dh = 10229045579369875490617726738896820556166874299729330716018741905455054848.000\n",
            "Layer 9, std of dl_dh = 1622769590787357221165998426801933037786842056906651889376730795250548736.000\n",
            "Layer 10, std of dl_dh = 291677266860870437780232576190188443241514488697868118675201045020278784.000\n",
            "Layer 11, std of dl_dh = 47040925004825710092577350412895155453321563697526018504489216478543872.000\n",
            "Layer 12, std of dl_dh = 7477794785556512561870544524514222904190343177295445298729337065832448.000\n",
            "Layer 13, std of dl_dh = 1079960630998983249752166647771489546517745439258609007782338718859264.000\n",
            "Layer 14, std of dl_dh = 177343205970746346967778093642488746598429707238845892914361820774400.000\n",
            "Layer 15, std of dl_dh = 29375585020824601862634145389942297584964020572455193261493213724672.000\n",
            "Layer 16, std of dl_dh = 4228152175740393840420430907377032438398605544578239798569609461760.000\n",
            "Layer 17, std of dl_dh = 571601369784794895910562631031416171904392915902935598268120301568.000\n",
            "Layer 18, std of dl_dh = 91625806080021454958264892378001309883763383380861328337156440064.000\n",
            "Layer 19, std of dl_dh = 12981649213666815470553036746972396917599455505424815878255411200.000\n",
            "Layer 20, std of dl_dh = 2521931090077676164675708087308560164602781204488721344485130240.000\n",
            "Layer 21, std of dl_dh = 366590511153710368970060455455171193527375669464669979272019968.000\n",
            "Layer 22, std of dl_dh = 51359326687770714591080421119603752588039560844573823207473152.000\n",
            "Layer 23, std of dl_dh = 8400447047412217384411313403421642231123427347819587759505408.000\n",
            "Layer 24, std of dl_dh = 1167175082942496437577952826235058182828232913615180278005760.000\n",
            "Layer 25, std of dl_dh = 189303733379321820949917225330147038267659861123596728401920.000\n",
            "Layer 26, std of dl_dh = 41712815717105573157388729547722781775721566276548869226496.000\n",
            "Layer 27, std of dl_dh = 5462460278894733452942109043961108684986345042145739538432.000\n",
            "Layer 28, std of dl_dh = 956779985561020314898718891153874492925720775421178413056.000\n",
            "Layer 29, std of dl_dh = 163553513870808605298394336984519469873821876163551690752.000\n",
            "Layer 30, std of dl_dh = 27962097471983048282779199579149906526177338456643993600.000\n",
            "Layer 31, std of dl_dh = 4533651840178385141827214279520261185924243910977126400.000\n",
            "Layer 32, std of dl_dh = 692775722996451643647634761606224467810881551541993472.000\n",
            "Layer 33, std of dl_dh = 115846381203838808661818838192193714359037102244495360.000\n",
            "Layer 34, std of dl_dh = 20622026877096518932624386617503886304556059665104896.000\n",
            "Layer 35, std of dl_dh = 3038521358601098311638948500818773455982912979599360.000\n",
            "Layer 36, std of dl_dh = 486644995502083127708682788488469787451507406274560.000\n",
            "Layer 37, std of dl_dh = 67762952239536214087718500711727746678730564567040.000\n",
            "Layer 38, std of dl_dh = 10332488588890049713609937944320962320259371499520.000\n",
            "Layer 39, std of dl_dh = 1461454311950454565864275500814865356181534146560.000\n",
            "Layer 40, std of dl_dh = 258241445087590776207952401654968831493980291072.000\n",
            "Layer 41, std of dl_dh = 40450451143651953145051042083546508464165486592.000\n",
            "Layer 42, std of dl_dh = 7712581729931044545150107431487218057337634816.000\n",
            "Layer 43, std of dl_dh = 1336239794865748360889709863959013995287937024.000\n",
            "Layer 44, std of dl_dh = 231495044789922145345902695211142088107229184.000\n",
            "Layer 45, std of dl_dh = 39500717745074512367426775088265226423894016.000\n",
            "Layer 46, std of dl_dh = 5987633018620223252680859241244423494828032.000\n",
            "Layer 47, std of dl_dh = 966020560711593611954353793080976121790464.000\n",
            "Layer 48, std of dl_dh = 140118487856732239819583266458026790354944.000\n",
            "Layer 49, std of dl_dh = 18817849764518896023941902245604242751488.000\n",
            "------------------------------------------------------------------------------------------------\n",
            "Layer 1, std of dl_dh = 38641616156682544227188473856.000\n",
            "Layer 2, std of dl_dh = 20374474729284212892901572608.000\n",
            "Layer 3, std of dl_dh = 11625657850642127889539006464.000\n",
            "Layer 4, std of dl_dh = 5696023189225340888741314560.000\n",
            "Layer 5, std of dl_dh = 2657382923492163039529009152.000\n",
            "Layer 6, std of dl_dh = 1382280815262033753350864896.000\n",
            "Layer 7, std of dl_dh = 722894256890225813226520576.000\n",
            "Layer 8, std of dl_dh = 323470823204855612021145600.000\n",
            "Layer 9, std of dl_dh = 162276959078736182449799168.000\n",
            "Layer 10, std of dl_dh = 92236450497310383417589760.000\n",
            "Layer 11, std of dl_dh = 47040925004825837394984960.000\n",
            "Layer 12, std of dl_dh = 23646863397689015553490944.000\n",
            "Layer 13, std of dl_dh = 10799606309989860010622976.000\n",
            "Layer 14, std of dl_dh = 5608084584239320096833536.000\n",
            "Layer 15, std of dl_dh = 2937558502082467887841280.000\n",
            "Layer 16, std of dl_dh = 1337059116913620886224896.000\n",
            "Layer 17, std of dl_dh = 571601369784796242247680.000\n",
            "Layer 18, std of dl_dh = 289746239661772598411264.000\n",
            "Layer 19, std of dl_dh = 129816492136668445278208.000\n",
            "Layer 20, std of dl_dh = 79750463466367424135168.000\n",
            "Layer 21, std of dl_dh = 36659051115371118460928.000\n",
            "Layer 22, std of dl_dh = 16241245142602731225088.000\n",
            "Layer 23, std of dl_dh = 8400447047412234584064.000\n",
            "Layer 24, std of dl_dh = 3690931690294239821824.000\n",
            "Layer 25, std of dl_dh = 1893037333793222426624.000\n",
            "Layer 26, std of dl_dh = 1319075052849236803584.000\n",
            "Layer 27, std of dl_dh = 546246027889474338816.000\n",
            "Layer 28, std of dl_dh = 302560397403584528384.000\n",
            "Layer 29, std of dl_dh = 163553513870808907776.000\n",
            "Layer 30, std of dl_dh = 88423916167102873600.000\n",
            "Layer 31, std of dl_dh = 45336518401783930880.000\n",
            "Layer 32, std of dl_dh = 21907491923386810368.000\n",
            "Layer 33, std of dl_dh = 11584638120383901696.000\n",
            "Layer 34, std of dl_dh = 6521257490083433472.000\n",
            "Layer 35, std of dl_dh = 3038521358601103872.000\n",
            "Layer 36, std of dl_dh = 1538906597708982016.000\n",
            "Layer 37, std of dl_dh = 677629522395363456.000\n",
            "Layer 38, std of dl_dh = 326741978385917632.000\n",
            "Layer 39, std of dl_dh = 146145431195045696.000\n",
            "Layer 40, std of dl_dh = 81663115273008896.000\n",
            "Layer 41, std of dl_dh = 40450451143652016.000\n",
            "Layer 42, std of dl_dh = 24389324906783776.000\n",
            "Layer 43, std of dl_dh = 13362397948657504.000\n",
            "Layer 44, std of dl_dh = 7320516085788504.000\n",
            "Layer 45, std of dl_dh = 3950071774507456.500\n",
            "Layer 46, std of dl_dh = 1893455813206931.500\n",
            "Layer 47, std of dl_dh = 966020560711595.125\n",
            "Layer 48, std of dl_dh = 443093563925919.312\n",
            "Layer 49, std of dl_dh = 188178497645189.219\n"
          ]
        }
      ],
      "source": [
        "# You can see that the gradients of the hidden units are increasing on average (the standard deviation is across all hidden units at the layer\n",
        "# and the 100 training examples\n",
        "\n",
        "# TO DO\n",
        "# Change this to 50 layers with 80 hidden units per layer\n",
        "# Number of layers\n",
        "K = 50\n",
        "# Number of neurons per layer\n",
        "D = 80\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "# Set variance of initial weights to 1\n",
        "sigma_sq_omega = 1.0\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
        "\n",
        "# For simplicity we'll just consider the gradients of the weights and biases between the first and last hidden layer\n",
        "n_data = 100\n",
        "aggregate_dl_df = [None] * (K+1)\n",
        "for layer in range(1,K):\n",
        "  # These 3D arrays will store the gradients for every data point\n",
        "  aggregate_dl_df[layer] = np.zeros((D,n_data))\n",
        "\n",
        "\n",
        "# We'll have to compute the derivatives of the parameters for each data point separately\n",
        "for c_data in range(n_data):\n",
        "  data_in = np.random.normal(size=(1,1))\n",
        "  y = np.zeros((1,1))\n",
        "  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
        "  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
        "  for layer in range(1,K):\n",
        "    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])\n",
        "\n",
        "for layer in range(1,K):\n",
        "  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))\n",
        "\n",
        "\n",
        "print(\"------------------------\"*4)\n",
        "\n",
        "\n",
        "# TO DO\n",
        "# Now experiment with sigma_sq_omega to try to stop the variance of the gradients exploding\n",
        "# You can see that the gradients of the hidden units are increasing on average (the standard deviation is across all hidden units at the layer\n",
        "# and the 100 training examples\n",
        "\n",
        "# TO DO\n",
        "# Change this to 50 layers with 80 hidden units per layer\n",
        "# Number of layers\n",
        "K = 50\n",
        "# Number of neurons per layer\n",
        "D = 80\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "# Set variance of initial weights to 1\n",
        "sigma_sq_omega = 0.1\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
        "\n",
        "# For simplicity we'll just consider the gradients of the weights and biases between the first and last hidden layer\n",
        "n_data = 100\n",
        "aggregate_dl_df = [None] * (K+1)\n",
        "for layer in range(1,K):\n",
        "  # These 3D arrays will store the gradients for every data point\n",
        "  aggregate_dl_df[layer] = np.zeros((D,n_data))\n",
        "\n",
        "\n",
        "# We'll have to compute the derivatives of the parameters for each data point separately\n",
        "for c_data in range(n_data):\n",
        "  data_in = np.random.normal(size=(1,1))\n",
        "  y = np.zeros((1,1))\n",
        "  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
        "  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
        "  for layer in range(1,K):\n",
        "    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])\n",
        "\n",
        "for layer in range(1,K):\n",
        "  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
